services:
  hive_app:
    container_name: hive_app
    build:
      context: .
      dockerfile: Dockerfile.ui
      args:
        REACT_APP_API_BACKED_URL: "http://localhost:8000" # This should match the backend below. NOTE: The backend can be ran indepdently.
        REACT_APP_API_MODEL_SELECTION: "/models/deepseek-33b" # This should match the --model you have set below in hive_backend.
        REACT_APP_API_MAX_GEN_TOKENS: 8192 # This should be a reasonable % of the max-model-len defined below.
        REACT_APP_API_MAX_TOKENS: 16384 # This should be equal to max-model-len defined below.
        REACT_APP_API_PROMPT: "You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer." # A prompt that will be sent at the beginning of the context window.
    ports:
      - "80:80" # This maps our UI front end to serve on port 80. Change as needed.

  hive_backend:
    container_name: hive_backend
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000" # This should match the port below. Optionally, you may map the host/internal port as needed. 
    ipc: "host" # This allows the container to access all your ram. See vLLM doc's for details
    command: 
      - --port=8000 # Internal port.
      - --host=0.0.0.0 # Internal network address.
      - --max-model-len=16384 # Set your context length. NOTE: This includes the context window..\
      - --download-dir=/models # The path where your model(s) are stored.
      - --model=/models/deepseek-33b # Your model name should folow this.
    volumes:
      - /data/models/deepseek-33b:/models/deepseek-33b # Please define this to where your model is located.
    deploy: # Currently vLLM only supports NVIDIA GPU's, you can define GPU utilization as needed here.
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]